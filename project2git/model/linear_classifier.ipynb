{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn import svm\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToTensor():\n",
    "\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def string_to_tensor(self, string_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A method to convert a string list to a tensor for a deep learning model\n",
    "        \"\"\"    \n",
    "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
    "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
    "        print('string_list',string_list)\n",
    "        return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data\n",
      "I AM HERE \n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train_pos=[]\n",
    "Y_train_pos=[]\n",
    "print('get data')\n",
    "f_pos=open('train_pos_full_lem.txt',encoding =\"latin-1\")\n",
    "for line in f_pos:\n",
    "    X_train_pos.append(line.strip())\n",
    "    Y_train_pos.append(1)\n",
    "\n",
    "X_train_neg=[]\n",
    "Y_train_neg=[]\n",
    "f_neg=open('train_neg_full_lem.txt',encoding =\"latin-1\")\n",
    "for line in f_neg:\n",
    "    X_train_neg.append(line.strip())\n",
    "    Y_train_neg.append(0)\n",
    "\n",
    "\n",
    "X_train=X_train_pos+X_train_neg\n",
    "Y_train=Y_train_pos+Y_train_neg\n",
    "Y_train=Y_train\n",
    "\n",
    "    \n",
    "\n",
    "# shuffle the data : \n",
    "print('I AM HERE ')\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "data = list(zip(X_train, Y_train))\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "X_train, labels = zip(*data)\n",
    "lables=list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924\n",
      "2312\n",
      "6119\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "14945\n",
      "20124\n",
      "21517\n",
      "22869\n",
      "23666\n",
      "26355\n",
      "27432\n",
      "27764\n",
      "29049\n",
      "40490\n",
      "43334\n",
      "44426\n",
      "46018\n",
      "46277\n",
      "46370\n",
      "47455\n",
      "47796\n",
      "50416\n",
      "51408\n",
      "52444\n",
      "57849\n",
      "60847\n",
      "63516\n",
      "65338\n",
      "66529\n",
      "69795\n",
      "70997\n",
      "73576\n",
      "75637\n",
      "75638\n",
      "75639\n",
      "76402\n",
      "78015\n",
      "78673\n",
      "78957\n",
      "81907\n",
      "87058\n",
      "95292\n",
      "96834\n",
      "99314\n",
      "99478\n"
     ]
    }
   ],
   "source": [
    "with open('../data/preprocessed/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "features=[]\n",
    "label=[]\n",
    "i=0\n",
    "\n",
    "embedded= np.load('../data/preprocessed/embeddings.npy')\n",
    "\n",
    "# Get our features and our labels \n",
    "with open('../data/preprocessed/train_pos_lem.txt',encoding =\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "        tokens = [t for t in tokens if t >= 0]\n",
    "        if len(tokens)==0:\n",
    "            features.append(np.zeros(20))\n",
    "            print(i)\n",
    "        else:\n",
    "            features.append(np.mean(embedded[tokens,:],0))\n",
    "        i+=1\n",
    "        label.append(1)\n",
    "        \n",
    "with open('../data/preprocessed/train_neg_lem.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "        tokens = [t for t in tokens if t >= 0]\n",
    "        if len(tokens)==0:\n",
    "            features.append(np.zeros(20))\n",
    "            \n",
    "        else:\n",
    "            features.append(np.mean(embedded[tokens,:],0))\n",
    "        i+=1\n",
    "        label.append(0)\n",
    "num_pos=i\n",
    "\n",
    "\n",
    "features=np.array(features)\n",
    "label=np.array(label)\n",
    "\n",
    "# Shuffle the data \n",
    "num_row = len(label)\n",
    "indices = np.random.permutation(num_row)\n",
    "\n",
    "features_shuffle=features[indices,:]\n",
    "label_shuffle=label[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Preprocess the data \\nwith open('tokenizer.pickle', 'rb') as f:\\n    tokenizer = pickle.load(f)\\nprint('create token')\\ntokens=tokenizer.texts_to_sequences(X_train)\\n\\nprint('load embedding matrix')\\nemb = np.load('../data/not_preprocessed/embeddings.npy')\\nfeatures=[]\\nfor i in range(len(X_train)):\\n    if len(tokens[i])==0 : #np.isnan(np.mean(emb[tokens[i],:])):\\n        features.append(np.zeros(200))\\n        print(tokens[i])\\n    else:\\n        features.append(np.mean(emb[tokens[i],:],0))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Preprocess the data \n",
    "with open('tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "print('create token')\n",
    "tokens=tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "print('load embedding matrix')\n",
    "emb = np.load('../data/not_preprocessed/embeddings.npy')\n",
    "features=[]\n",
    "for i in range(len(X_train)):\n",
    "    if len(tokens[i])==0 : #np.isnan(np.mean(emb[tokens[i],:])):\n",
    "        features.append(np.zeros(200))\n",
    "        print(tokens[i])\n",
    "    else:\n",
    "        features.append(np.mean(emb[tokens[i],:],0))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_shuffle=np.array(features_shuffle)\n",
    "label_shuffle=np.array(label_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train our model\n",
      "0.597855\n"
     ]
    }
   ],
   "source": [
    "print('train our model')\n",
    "clf =  svm.SVC(kernel='linear') #sklearn.linear_model.LogisticRegression()\n",
    "clf.fit(features_shuffle, label_shuffle)\n",
    "print(clf.score(features_shuffle, label_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'model_LogReg_lem.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
