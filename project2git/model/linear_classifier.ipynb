{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924\n",
      "2312\n",
      "6119\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "14945\n",
      "20124\n",
      "21517\n",
      "22869\n",
      "23666\n",
      "26355\n",
      "27432\n",
      "27764\n",
      "29049\n",
      "40490\n",
      "43334\n",
      "44426\n",
      "46018\n",
      "46277\n",
      "46370\n",
      "47455\n",
      "47796\n",
      "50416\n",
      "51408\n",
      "52444\n",
      "57849\n",
      "60847\n",
      "63516\n",
      "65338\n",
      "66529\n",
      "69795\n",
      "70997\n",
      "73576\n",
      "75637\n",
      "75638\n",
      "75639\n",
      "76402\n",
      "78015\n",
      "78673\n",
      "78957\n",
      "81907\n",
      "87058\n",
      "95292\n",
      "96834\n",
      "99314\n",
      "99478\n"
     ]
    }
   ],
   "source": [
    "with open('../data/preprocessed/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "features=[]\n",
    "label=[]\n",
    "\n",
    "embedded= np.load('../data/preprocessed/embeddings.npy')\n",
    "\n",
    "# Get our features and our labels \n",
    "with open('../data/preprocessed/train_pos_lem.txt',encoding =\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "        tokens = [t for t in tokens if t >= 0]\n",
    "        if len(tokens)==0:\n",
    "            features.append(np.zeros(20))\n",
    "        else:\n",
    "            features.append(np.mean(embedded[tokens,:],0))\n",
    "        label.append(1)\n",
    "        \n",
    "with open('../data/preprocessed/train_neg_lem.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "        tokens = [t for t in tokens if t >= 0]\n",
    "        if len(tokens)==0:\n",
    "            features.append(np.zeros(20))\n",
    "            \n",
    "        else:\n",
    "            features.append(np.mean(embedded[tokens,:],0))\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "\n",
    "features=np.array(features)\n",
    "label=np.array(label)\n",
    "\n",
    "# Shuffle the data \n",
    "num_row = len(label)\n",
    "indices = np.random.permutation(num_row)\n",
    "\n",
    "features_shuffle=features[indices,:]\n",
    "label_shuffle=label[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_shuffle=np.array(features_shuffle)\n",
    "label_shuffle=np.array(label_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train our model\n",
      "0.62467\n"
     ]
    }
   ],
   "source": [
    "print('train our model')\n",
    "# lem : 0.593235\n",
    "# nolem : 0.588245\n",
    "\n",
    "# best parameters : \n",
    "# linear svc : # {'C': 1, 'loss': 'hinge', 'penalty': 'l2'}\n",
    "# logistic regression : # {'C': 10, 'penalty': 'l2'}\n",
    "# SVC : 'rbf': 0.62465\n",
    "#       'poly': 0.60739\n",
    "\n",
    "clf = svm.SVC(kernel='rbf') #sklearn.linear_model.LogisticRegression(C=10,penalty= 'l2') #svm.SVC(kernel='linear') \n",
    "clf.fit(features_shuffle, label_shuffle)\n",
    "print(clf.score(features_shuffle, label_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'model_SVCrbf_lem.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': (1,10), 'penalty' : ('l1', 'l2'), 'loss' : ('hinge', 'squared_hinge')}  \n",
    "linearSVC_ = GridSearchCV(svm.LinearSVC(), param_grid, cv=4, return_train_score=True)\n",
    "linearSVC_.fit(features_shuffle, label_shuffle)\n",
    "print(linearSVC_.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel': ('rbf','poly')}  \n",
    "SVC_ = GridSearchCV(svm.SVC(), param_grid, cv=4, return_train_score=True)\n",
    "SVC_.fit(features_shuffle, label_shuffle)\n",
    "print(SVC_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': (1,10), 'penalty' : ('l1', 'l2')}  \n",
    "Logreg_ = GridSearchCV(sklearn.linear_model.LogisticRegression(), param_grid, cv=4, return_train_score=True)\n",
    "Logreg_.fit(features_shuffle, label_shuffle)\n",
    "print(Logreg_.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
