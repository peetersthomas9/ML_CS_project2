{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation from this site : https://medium.com/analytics-vidhya/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore')) # extract embedding vectors from our glove dataset\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, max_features):\n",
    "        \n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix\n",
    "    \n",
    "class TextToTensor():\n",
    "    \n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def string_to_tensor(self, string_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A method to convert a string list to a tensor for a deep learning model\n",
    "        \"\"\"    \n",
    "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
    "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
    "        print('string_list',string_list)\n",
    "        return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokenization_embedding(X_train,Y_train,embed_path,embed_dim=200,max_len=20,num_words=200000):\n",
    "    # Preprocecing the text\n",
    "    print('Preprocecing the text')\n",
    "\n",
    "    Y_train = np.asarray(Y_train)\n",
    "\n",
    "\n",
    "    # Tokenizing the text\n",
    "    print('Tokenizing the text')\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    if len(tokenizer.word_counts)<num_words:\n",
    "        num_words=len(tokenizer.word_counts)\n",
    "    tokenizer.num_words=num_words\n",
    "\n",
    "    #with open('tokenizer.pickle', 'wb') as handle:\n",
    "    #    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Creating the embedding matrix\n",
    "    print('Creating the embedding matrix')\n",
    "    embedding = Embeddings(embed_path, embed_dim)\n",
    "\n",
    "    embedding_matrix = embedding.create_embedding_matrix(tokenizer, num_words)\n",
    "\n",
    "    save=False # if we want to save the embedding matrix and the tokenizer \n",
    "    if save : \n",
    "        with open('tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        np.save('embeddings_matrix', embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data\n",
      "I AM HERE \n",
      "Preprocecing the text\n",
      "Y train after [1 1 1 ... 0 1 0]\n",
      "Tokenizing the text\n",
      "Creating the embedding matrix\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "X_train_pos=[]\n",
    "Y_train_pos=[]\n",
    "print('get data')\n",
    "f_pos=open('train_pos_full_lem.txt',encoding =\"latin-1\")\n",
    "for line in f_pos:\n",
    "    X_train_pos.append(line.strip())\n",
    "    Y_train_pos.append(1)\n",
    "\n",
    "X_train_neg=[]\n",
    "Y_train_neg=[]\n",
    "f_neg=open('train_neg_full_lem.txt',encoding =\"latin-1\")\n",
    "for line in f_neg:\n",
    "    X_train_neg.append(line.strip())\n",
    "    Y_train_neg.append(0)\n",
    "\n",
    "\n",
    "X_train=X_train_pos+X_train_neg\n",
    "Y_train=Y_train_pos+Y_train_neg\n",
    "Y_train=Y_train\n",
    "\n",
    "    \n",
    "\n",
    "# shuffle the data : \n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "data = list(zip(X_train, Y_train))\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "X_train, Y_train = zip(*data)\n",
    "\n",
    "max_len = 25\n",
    "embed_path='../data/glove.twitter.27B.200d.txt'\n",
    "\n",
    "compute_tokenization_embedding(\n",
    "X_train,\n",
    "Y_train,\n",
    "embed_path,\n",
    "embed_dim=25,\n",
    "max_len=max_len,\n",
    "num_words=200000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
