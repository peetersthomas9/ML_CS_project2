{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1638117428783,
     "user": {
      "displayName": "thomas peeters",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03568489975844982270"
     },
     "user_tz": -60
    },
    "id": "96pRcJNmabkH",
    "outputId": "ac77c2c0-300d-47a8-bf0b-e97eb2316ee3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peete\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WGe64nbSc4Wf"
   },
   "outputs": [],
   "source": [
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()  # used for lematization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8BjcKdYVbraG"
   },
   "outputs": [],
   "source": [
    "# documentation from this site : https://medium.com/analytics-vidhya/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing numbers : \n",
    "    string = re.sub(r'[0-9]+', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "\n",
    "    # reduce length of words : ex caaaar -> caar  + lemming : \n",
    "    \n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    words=string.split()\n",
    "\n",
    "    new_words = [wordnet_lemmatizer.lemmatize(pattern.sub(r\"\\1\\1\", word), pos=\"v\") for word in words]\n",
    "    \n",
    "    sent_str = \"\"\n",
    "    for i in new_words:\n",
    "        sent_str += str(i) + \" \"\n",
    "   \n",
    "    return sent_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_clean(file,file_output):\n",
    "    # function to clean the text \n",
    "    X_train_neg=[]\n",
    "    Y_train_neg=[]\n",
    "    print('get data')\n",
    "    f_neg=open(file,encoding =\"utf8\")\n",
    "    for line in f_neg:\n",
    "        X_train_neg.append(line.strip())\n",
    "        Y_train_neg.append(1)\n",
    "\n",
    "    print('Preprocecing the text ')\n",
    "    X_new=[]\n",
    "    num=0\n",
    "    for text in X_train_neg:\n",
    "      X_new.append(clean_text(text))\n",
    "      num=num+1\n",
    "      if (num%10000)==0:\n",
    "        print(num)\n",
    "\n",
    "    text_file = open(file_output,\"w\", encoding =\"utf8\")\n",
    "    for line in X_new:\n",
    "        text_file.write(line+'\\n')\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451102,
     "status": "ok",
     "timestamp": 1638119174238,
     "user": {
      "displayName": "thomas peeters",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03568489975844982270"
     },
     "user_tz": -60
    },
    "id": "64bA7KBybsC9",
    "outputId": "e2319bca-02fb-4415-dd7c-ec3c26e99d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data\n",
      "Preprocecing the text \n",
      "10000\n",
      "train pos\n",
      "get data\n",
      "Preprocecing the text \n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "train pos_full\n",
      "get data\n",
      "Preprocecing the text \n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "train neg\n",
      "get data\n",
      "Preprocecing the text \n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "train neg_ful\n",
      "get data\n",
      "Preprocecing the text \n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n"
     ]
    }
   ],
   "source": [
    "# Apply the function of clean text to all the files \n",
    "\n",
    "file='../data/not_preprocessed/test_data.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/test_data_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "\n",
    "print('train pos')\n",
    "file='../data/not_preprocessed/train_pos.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_pos_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "print('train pos_full')\n",
    "file='../data/not_preprocessed/train_pos_full.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_pos_full_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "print('train neg')\n",
    "file='../data/not_preprocessed/train_neg.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_neg_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "print('train neg_ful')\n",
    "file='../data/not_preprocessed/train_neg_full.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_neg_full_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU5orosIdzYT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyp9k/vaENdgzRUEeljVoS",
   "collapsed_sections": [],
   "name": "clean_text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
