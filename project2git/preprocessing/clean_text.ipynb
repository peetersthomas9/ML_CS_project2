{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4864,
     "status": "ok",
     "timestamp": 1638110228566,
     "user": {
      "displayName": "thomas peeters",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03568489975844982270"
     },
     "user_tz": -60
    },
    "id": "oxOH1Crwd69V",
    "outputId": "a157965d-8252-4dec-8c10-7a98bcccdb5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\peete\\anaconda3\\lib\\site-packages (2.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1638117428783,
     "user": {
      "displayName": "thomas peeters",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03568489975844982270"
     },
     "user_tz": -60
    },
    "id": "96pRcJNmabkH",
    "outputId": "ac77c2c0-300d-47a8-bf0b-e97eb2316ee3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peete\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WGe64nbSc4Wf"
   },
   "outputs": [],
   "source": [
    "spell = Speller(lang='en')\n",
    "ps = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8BjcKdYVbraG"
   },
   "outputs": [],
   "source": [
    "# documentation from this site : https://medium.com/analytics-vidhya/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing numbers : \n",
    "    string = re.sub(r'[0-9]+', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "\n",
    "    # reduce length of words : ex caaaar -> caar  + stemming : \n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    words=string.split()\n",
    "\n",
    "    new_words = [wordnet_lemmatizer.lemmatize(pattern.sub(r\"\\1\\1\", word), pos=\"v\") for word in words]\n",
    "    \n",
    "    sent_str = \"\"\n",
    "    for i in new_words:\n",
    "        sent_str += str(i) + \" \"\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_clean(file,file_output):\n",
    "    X_train_neg=[]\n",
    "    Y_train_neg=[]\n",
    "    print('get data')\n",
    "    f_neg=open(file,encoding =\"utf8\")\n",
    "    for line in f_neg:\n",
    "        X_train_neg.append(line.strip())\n",
    "        Y_train_neg.append(1)\n",
    "\n",
    "    print('Preprocecing the text ')\n",
    "    X_new=[]\n",
    "    num=0\n",
    "    for text in X_train_neg:\n",
    "      X_new.append(clean_text(text))\n",
    "      num=num+1\n",
    "      if (num%10000)==0:\n",
    "        print(num)\n",
    "\n",
    "    text_file = open(file_output,\"w\", encoding =\"utf8\")\n",
    "    for line in X_new:\n",
    "        text_file.write(line+'\\n')\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451102,
     "status": "ok",
     "timestamp": 1638119174238,
     "user": {
      "displayName": "thomas peeters",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03568489975844982270"
     },
     "user_tz": -60
    },
    "id": "64bA7KBybsC9",
    "outputId": "e2319bca-02fb-4415-dd7c-ec3c26e99d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data\n",
      "Preprocecing the text \n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "file='../data/not_preprocessed/test_data.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/test_data_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "\n",
    "file='../data/not_preprocessed/train_pos.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_pos_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "\n",
    "file='../data/not_preprocessed/train_pos_full.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_pos_full_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "\n",
    "file='../data/not_preprocessed/train_neg.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_neg_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)\n",
    "\n",
    "file='../data/not_preprocessed/train_neg_full.txt' # select the file we want to pre-process \n",
    "file_output='../data/preprocessed/train_neg_full_lem.txt'\n",
    "\n",
    "compute_text_clean(file,file_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU5orosIdzYT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyp9k/vaENdgzRUEeljVoS",
   "collapsed_sections": [],
   "name": "clean_text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
